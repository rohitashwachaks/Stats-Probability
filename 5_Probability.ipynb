{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "5. Probability.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSsDxmAZkhUN"
      },
      "source": [
        "# Probability\n",
        "\n",
        "### Learning Objectives:\n",
        "- [Random Variables](#Random-Variables)\n",
        "- [Introduction to Probability](#Introduction-to-Probability)\n",
        "- [Joint & Marginal Probability](#Joint-&-Marginal-Probability)\n",
        "- [Conditional Probability](#Conditional-Probability)\n",
        "- [Probability Chain Rule](#Probability-Chain-Rule)\n",
        "- [Expected Value & Variance](#Expected-Value-&-Variance)\n",
        "- [Estimation: Variance & Bias](#Estimation:-Variance-&-Bias)\n",
        "\n",
        "# Random Variables\n",
        "A __random variable__ is defined as a variable whose values depend on outcomes of random events. For example, when we carry out an experiment, we never get _exactly_ the same result twice, maybe due to some measurement errors or some flaws in the system we are using to measure. But what is __randomness__? \n",
        "\n",
        "Some argue that this variation is not random and simply due to high-complexity processes that we are in no place of following accurately. For example, Robert Brown, a famous physicist, saw that pollen particles in water seem to bounce around randomly by bouncing about the water molecules. Some would argue that this was not random, and that if we knew where __all__ water molecules were and where they were going at all times, we could predict exactly where they would be later on. However, this is impossible to do in practice, and there was far more success when treating these as random forces!\n",
        "\n",
        "How do we generate a random value? We can use in-built functions, such as the ones from the __random__ and __numpy__ modules to generate random values. Numpy comes with the added functionality of creating vectors and matrices of random values in one line:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kc3FdwFtkhUW"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Generating randomness with random!\n",
        "random_number = random.randint(0,4) # random integer between 0 and 4\n",
        "random_number2 = random.random() # random floating point value between 0 and 1\n",
        "print(\"random module:\", random_number, random_number2)\n",
        "\n",
        "# Generating randomness with numpy!\n",
        "random_number = np.random.uniform(0,4) # random value from uniform distribution between 0 and 4\n",
        "random_number2 = np.random.normal(0,1) # random value from standard normal distribution\n",
        "print(\"numpy module:\",random_number, random_number2)\n",
        "\n",
        "print()\n",
        "\n",
        "# Generating a random vector and matrix\n",
        "random_vector = np.random.normal(0,1,5) # 1x5 vector from standard normal distribution\n",
        "random_matrix = np.random.uniform(0,4,(2,2)) # 2x2 matrixrandom values from uniform distribution between 0 and 4\n",
        "print(random_vector)\n",
        "print(random_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gNzDoFmkhUb"
      },
      "source": [
        "When it comes to random variables, we cannot predict exactly what values they will take, but rather, we focus on determining on the likelihood of taking different values using various statistical tools. But how do we quantify ___how likely___ a random variable is to take different values?\n",
        "\n",
        "\n",
        "#  Introduction to Probability\n",
        "__Probability__ is a measure of how likely an outcome is to occur given all other possible outcomes and the given circumstances. Therefore, when dealing with random variables, we do not concern ourselves with __what will happen__, but instead with __the probability of given outcome(s), also known as events, occuring__. Probability is formally defined as:\n",
        "\n",
        "$$ \\text{Probability of an outcome} = \\frac{\\text{Number of wanted outcomes}}{\\text{Number of possible outcomes}} $$\n",
        "\n",
        "Intuitively, we know that the probability of something occurring has to be somewhere between 0, where the outcome _cannot_ occur, and 1, where the outcome _will_ occur, where everything else is somewhere in between. We can apply probability in the context of the likelihood of a random variable taking a value or being within a range of values.\n",
        "\n",
        "\n",
        "In general, we use the operator $P(A)$ to denote the probability that an event A takes place. Let us look at an example with random variables. Given a random variable X, and a value $x_{1}$, we can  define $P(X\\geq x_{1}\\cup X\\leq x_{1})$ as the probability of ___either___ $X\\geq x_{1}$ or $X\\leq x_{1}$ taking place and $P(X\\geq x_{1}\\cap X\\leq x_{1})$ as the probability of ___both___ $X\\geq x_{1}$ and $X\\leq x_{1}$. In terms of logical operators, __union__( $\\cup$ ) corresponds to the __or__ operator and __intersection__( $\\cap$ ) to the __and__ operator. Using these, we can define key properties of probability theory: \n",
        "\n",
        "$$1. P(X=x_{1}) = 1-P(X=x_{1}') $$\n",
        "$$2. \\sum_{i=1}^{N}P(X=x_{i}) = 1$$\n",
        "\n",
        "<img src=\"images/venn.png\" alt=\"tree-diagram\"\n",
        "\ttitle=\"Tree diagram of the bolt production process\" width=\"500px\" height=\"350px\" />\n",
        "    \n",
        "\n",
        "$X=x_{1} '$ is the __complement__ of $X=x_{1}$, and represents all other values X can take besides $x_{1}$. The complement behaves like the __not__ operator, meaning the first property implies that the probability of a value is equal to 1 minus the probability of __NOT__ getting that value. If the probability of raining tomorrow is 0.8, then the probability that it will NOT rain tomorrow is 1 - 0.8 = 0.2. The second property implies that the sum of the probabilities of all values a random variable can take must equal to 1. This makes sense, since out of all possible outcomes, at least one must take place. For example, there is a probability of 1 that it will either rain or not tomorrow, and there is a probability of 1 that the height of a human we pick at random will be between 0 and infinity! If the two conditions cannot be true simultaneously, they are known as __mutually exclusive__. For instance, it cannot rain _and_ not rain at the same time, so these are mutually exclusive events. \n",
        "\n",
        "\n",
        "So when we look at the probability in the context of random variables, we can determine the probability of a random variable taking different values. However, we can also consider the probability of two or more random variables simultaneously.\n",
        "\n",
        "\n",
        "# Joint & Marginal Probability\n",
        "\n",
        "Rather than only considering the probabilities associated with one random variable, we can also look at the probabilities associated with a pair or set of random variables at specific values. The probability of multiple random variables taking specific values is known as a __joint probability__. This is an interesting property in understanding the relationship between these variables. If we consider two random variables, X and Y, their joint probability is, in general, denoted as follows:\n",
        "\n",
        "$$p(x,y) = P(X=x \\;and\\; Y=y)$$\n",
        "\n",
        "\n",
        "Consider the example data shown below, based on the random variables \"AI Core Graduate\" and \"Jobs In Data Science\":\n",
        "\n",
        "|                               | AI Core Graduate | Non-AI Core Graduate | Total |\n",
        "|-------------------------------|------------------|----------------------|-------|\n",
        "| Jobs In Data Science    | 120              | 30                   | 150   |\n",
        "| No Jobs In Data Science | 0                | 150                  | 150   |\n",
        "| Total                         | 120              | 180                  | 300   |\n",
        "\n",
        "We have seen above that to calculate a probability, we simply divide the number of wanted outcomes by the total number of outcomes. Therefore, to calculate the probability of each individual combination of values of our random variables, we divide by the total number of outcomes, which in this case is 300. This leads to the table below:\n",
        "\n",
        "|                               | AI Core Graduate | Non-AI Core Graduate | Total |\n",
        "|-------------------------------|------------------|----------------------|-------|\n",
        "| Jobs In Data Science    | 0.4              | 0.1                  | 0.5   |\n",
        "| No Jobs In Data Science | 0.0              | 0.5                  | 0.5   |\n",
        "| Total                         | 0.4              | 0.6                  | 1     |\n",
        "\n",
        "This is now a table of __joint probabilities__. Let us rewrite these results in their general form:\n",
        "\n",
        "$$p(\\text{AI Core Graduate, Jobs In Data Science}) = 0.4$$\n",
        "\n",
        "$$p(\\text{AI Core Graduate, No Jobs In Data Science}) = 0.0$$\n",
        "\n",
        "$$p(\\text{Non-AI Core Graduate, Jobs In Data Science}) = 0.1$$\n",
        "\n",
        "$$p(\\text{Non-AI Core Graduate, No Jobs In Data Science}) = 0.5$$\n",
        "\n",
        "This example helps us understand the importance of joint probailities, as in this case, the different joint probabilities tell us that it is way more likely to have a job in data science if you are an AI Core graduate than if you are not a graduate!\n",
        "\n",
        "However, sometimes we are only concerned with probability of one of the two (or more) random variables taking a particular variable. For example, what if we wanted to know from our data what the probability that someone being an AI Core graduate is? what is the probability of someone having a job in data science, regardless of whether they've studied with us or not? These are known as __marginal probabilities.__ In general, for two random variables, X and Y, we can calculate their marginal probabilities, $p_{X}(x)$ and $p_{Y}(y)$, as follows:\n",
        "\n",
        "$$p_{X}(x) = \\sum_{y}p(x,y)$$\n",
        "\n",
        "$$p_{Y}{y} = \\sum_{x}p(x,y)$$\n",
        "\n",
        "This means that for that marginal probability of a random variable X, we simply __sum joint probabilities of all possible values that other random variables can take__. The reverse applies for the case of the random variable Y. This means that in our case, we can calculate the marginal probabilities as follows:\n",
        "\n",
        "$$p(\\text{AI Core Graduate}) = p(\\text{AI Core Graduate, Jobs In Data Science}) + p(\\text{AI Core Graduate, No Jobs In Data Science}) = 0+ 0.4 = 0.4$$\n",
        "\n",
        "$$p(\\text{Jobs In Data Science}) = p(\\text{AI Core Graduate, Jobs In Data Science}) + p(\\text{Non-AI Core Graduate, Jobs In Data Science}) = 0.4 + 0.1 = 0.5$$\n",
        "\n",
        "\n",
        "# Conditional Probability\n",
        "While random variables, are, as the name implies, random, probabilities are the measure of the likelihood of an event occuring. Under different circumstances, things may become more or less likely to happen. Given that it is sunny now, the probability of raining soon is smaller than if it was cloudy. This leads to the field of __conditional probability__, which is the probability of an event taking place ___given___ another event has occured. In the context of the joint probability of two random varaibles, X and Y, we consider $P(X=x|Y=y)$ to represent the probability of $X=x$ occuring _given that_ $Y=y$, which we can define it with the equation below.\n",
        "\n",
        "$$P(X=x|Y=y) = p(x|y) = \\frac{P(X=x\\cap Y=y)}{P(Y=y)} = \\frac{p(x,y)}{p_{Y}(y)}$$\n",
        "\n",
        "While there is no formal proof for the equation above, we can make intuitive sense of it. There is a probability associated with the uncertainty of whether $Y=y$ or not. So if we assume that X is __dependent__ on Y, meaning the probability of X taking a given value depends on what value Y will take, knowing the value of Y removes some uncertainty from what value X will take. \n",
        "\n",
        "For example, let's say that I'm a birdwatcher following a rare yellow flamingo. My goal is to find the flamingo, and take a picture of him that we can publish on the _BirdsBirdsBirds Weekly_ magazine. For my success, both the event of finding it and getting a good picture need to take place. However, if I have already found the yellow flamingo, the probability of getting a good picture that day increases proportionally to how hard it was to find it in the first place!\n",
        "\n",
        "We can also establish the __law of total probability__ by elaborating on the equation for marginal probability, which is given by:\n",
        "\n",
        "$$p_{X}(x) = \\sum_{i=1}^{N}p(x|y_{i})p_{Y}(y_{i}) $$\n",
        "\n",
        "Where N is the total number of prior outcomes, $y_{i}$ is the $i^{th}$ possible value of Y. If we analyse the equation more closely, we can see that the sum of the probabilities of $y_{i}$ and $x$ both occuring are the individual components that make up the probability of $X=x$ taking place. \n",
        "\n",
        "For example, let's assume that if I don't find a flamingo, I can still get a picture of one from one of my sources. The probability of me getting a picture of a yellow flamingo is just the sum of the probability of _finding_ a flamingo and getting a picture and _not finding_ a flamingo and getting a picture.\n",
        "\n",
        "The law of total probability enables us to derive another incredibly useful theorem, known as __Bayes' Theorem__, which is used for revising predictions (updating probabilities) given additional evidence. Bayes Theorem is given as follows:\n",
        "\n",
        "$$p(x_{j}|y) = \\frac{p(x_{j}, y)}{p_{Y}(y)} = \\frac{p(y, x_{j})}{p_{Y}(y)}=  \\frac{p(y|x_{j})p(x_{j})}{\\sum_{i=1}^{N}p(y|x_{i})p(x_{i})} $$\n",
        "\n",
        "This equation dictates that the probability of the prior outcome $X=x_{j}$ having taken place given that the $Y=y$ has _now_ taken place is given by the ratio between the probability that $X=x_{j}$ and $Y=y$ occured and the probability that $Y=y$ followed any possible event $X=x_{i}$.\n",
        "\n",
        "We will now look at a quick example to understand these concepts in practice, shown in the __tree diagram__ below, with the properties:\n",
        "- Imagine a type of bolt that can be produced either in factory A or factory B. They sometimes end up defective.\n",
        "- 60% of bolts are produced in A and 40% of bolts are produced in B\n",
        "- 2% of bolts produced in A are defective and 4% of bolts produced in B are defective\n",
        "\n",
        "We can model this situation with two random variables:\n",
        "- X: can either take the value 'A' or 'B', corresponding to which factory the bolt was produced in\n",
        "- Y: can either take the value 'D' or 'D'' corresponding to whether it is defective or not\n",
        "\n",
        "<img src=\"images/tree.png\" alt=\"tree-diagram\"\n",
        "\ttitle=\"Tree diagram of the bolt production process\" width=\"750px\" height=\"500px\" />\n",
        "    \n",
        "Given the diagram above and the process, we can answer the following questions:\n",
        "1. What is the probability that the bolt is from factory A and it is defective? <br>\n",
        "$p(A, D) = p(D|A)p_{X}(A) = 0.02\\cdot 0.6 = 0.012 $\n",
        "2. Using the law of total probability, what is the probability that a bolt is defective? <br>\n",
        "$p_{Y}(D) = \\sum_{i=1}^{N}p(D|x_{i})p_{X}(x_{i}) = p(D|A)p_{X}(A) + p(D|B)p_{X}(B) = 0.02\\cdot 0.6 + 0.04\\cdot 0.4 = 0.028 $\n",
        "3. Using Bayes Theorem, what is the probability that a bolt is from factory B, given that it is defective? <br>\n",
        "$p(B|D) = \\frac{p(D|B)p_{X}(B)}{\\sum_{i=1}^{N}p(D|x_{i})p_{X}(x_{i})} = \\frac{0.04\\cdot 0.4}{0.04\\cdot 0.4 + 0.02\\cdot 0.6} = 0.57$\n",
        "4. Using Bayes Theorem, what is the probability that a bolt is from factory A, given that it is defective? <br>\n",
        "$P(A|D) = \\frac{P(D|A)P(A)}{\\sum_{i=1}^{N}P(D|A_{i})P(x_{i})} = \\frac{0.02\\cdot 0.6}{0.04\\cdot 0.4 + 0.02\\cdot 0.6} = 0.43$\n",
        "\n",
        "So with the laws of probability we've gone over, we were able to calculate the probability that a bolt is defective no matter where it came from originally and even what factories are responsible for the majority of the defective bolts, which in this case is factory B!\n",
        "\n",
        "Similarly, if the occurance of an event does not affect the probability of another, implying $P(x|y) = P(x)$, X and Y are __independent random variables__. If I eat a croissant for breakfast today, this will not affect the probability of it raining in two weeks time. Given the above equation, we can conclude that for independent random variables X and Y:\n",
        "$$p(x,y) = p_{X}(x)p_{Y}(y)$$\n",
        "\n",
        "\n",
        "# Probability Chain Rule\n",
        "The __probability chain rule__ is a crucial concept in probability, and is based on the expansion of the concepts we covered in conditional probability so far. It is used for computing the joint probability of any number of random variables based on only their conditional probabilities. We have already looked at the case of two random variables, so for now, let us cover what happens for three random variables X, Y and Z. The probability of $X=x$, $Y=y$ and $Z=z$ taking place can be computed as the probability of $X=x$ _given that_ $Y=y$ and $Z=z$ have taken place times the probability of $Y=y$ and $Z=z$ taking place. This is shown below:\n",
        "\n",
        "$$P(X=x, Y=y, Z=z) = p(x,y,z) = p(x|y,z)p(y,z)$$\n",
        "\n",
        "We can do this by in that particular case, treating the values of \"Y and Z\" one outcome, then applying the relationship for conditional probabilities and joint probabilities we have previously seen:\n",
        "\n",
        "$$p(x,y,z) = p(x|y,z)p(y,z) = p(x|y,z)p(y|z)p_{Z}(z)$$\n",
        "\n",
        "To nail down this concept we will also cover this process for the case of the joint probability of four random variables, X, Y, Z, T. We start by initially treating the values of (Y, Z, T) as one outcome:\n",
        "\n",
        "$$p(x,y,z,t) = p(x|y,z,t)p(y,z,t)$$\n",
        "\n",
        "We now have another joint probability of three random variables, which we can substitute by what we got above:\n",
        "\n",
        "$$p(x,y,z,t) = p(x|y,z,t)p(y,z,t) = p(x|y,z,t)p(y|z,t)p(z|t)p_{T}(t)$$\n",
        "\n",
        "For cases with more than four random variables, we can apply the same logic recursively:\n",
        "- Treat all the values of all but one random variable as one outcome\n",
        "- Re-write the joint probability as a conditional probability times the joint probability of one less variable\n",
        "- Do the same process for the new joint probability until we get to our base case of two random variables\n",
        "\n",
        "We can generalize the above results. For any N random variables $X_{1},X_{2},...,X_{N}$, we can use the chain rule of probability to re-write their joint probability (given the reasoning above) in the following manner:\n",
        "\n",
        "$$p(x_{1},x_{2},...,x_{N}) = \\prod_{i=1}^{N}p(x_{i}|x_{1},...,x_{i-1})$$\n",
        "\n",
        "And that is the gist of it! This is an important tool in data science and machine learning, and is crucial when dealing with Bayesian Networks!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PimekUDkhUn"
      },
      "source": [
        "# Expected Value & Variance\n",
        "\n",
        "As we will see in later chapters in the field of descriptive statistics, we can estimate the __expected value__, $\\mathbf{E(X)}$ and __variance,__ $\\mathbf{Var(X)}$, of a random variable $X$ given $N$ samples as follows:\n",
        "\n",
        "$$E(X) \\approx \\bar{x} = \\frac{1}{N} \\sum_{i=1}^{N}x_{i}$$\n",
        "$$Var(X) \\approx s ^{2} = \\frac{1}{N} \\sum_{i=1}^{N}(\\bar{x} - x_{i})^{2}$$\n",
        "\n",
        "Where E(.) is known as the __expectation operator__, and returns the expected value of the random variable we apply it to, and Var(.) is the __variance operator__, and returns the variance of the random variable we apply it to. Below we show the implementation of these equations in standard programming and NumPy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkjhFyoNkhUq"
      },
      "source": [
        "# Defining our dataset\n",
        "X = [1, 3, 5, 10, 20]\n",
        "\n",
        "def mean(X):\n",
        "    mean = 0\n",
        "    for x in X:\n",
        "        mean += x\n",
        "    mean /= len(X)\n",
        "    return mean\n",
        "\n",
        "def var(X):\n",
        "    x_bar = mean(X)\n",
        "    var = 0\n",
        "    for x in X:\n",
        "        var += (x_bar - x)**2\n",
        "    var /= len(X)\n",
        "    return var\n",
        "    \n",
        "mean1 = mean(X)\n",
        "var1 = var(X)\n",
        "\n",
        "X = np.array(X)\n",
        "mean2 = np.mean(X)\n",
        "var2 = np.var(X)\n",
        "\n",
        "print(\"Mean:\")\n",
        "print(\"Programming Mean:\", mean1)\n",
        "print(\"NumPy Mean:\", mean2)\n",
        "print()\n",
        "print(\"Variance:\")\n",
        "print(\"Programming Variance:\", var1)\n",
        "print(\"NumPy Variance:\", var2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPLv_j5ikhUr"
      },
      "source": [
        "The expected value of a random variable is a __measure of central tendency__, and is a single value that aims to describe the _center_ of the possible values a random variable can take. The variance of a random variable is a __measure of dispersion__ and is used as a way to quantify how spread the values of a random variable are. In statistics, probability is intrinsically linked to the calculation of the expected value and variance of a random variable. Before we see this link, we will need to define a __probability distribution__.\n",
        "\n",
        "The _probability distribution_ of a random variable is a function that takes a possible value of the random variable as an input and returns the probability of taking that value. To understand this in more detail, we will go over the example of throwing a dart. If we consider the random variable $X$ to be the money (£) the area where our dart lands rewards us in this beautifully labelled dartboard, we get the probability distribution below:\n",
        "\n",
        "<img src=\"images/dart.png\" alt=\"tree-diagram\"\n",
        "\ttitle=\"Dart throw\" width=\"500px\" height=\"350px\" />\n",
        "\n",
        "| Probability Distribution of Dart Throw Points |      |      |      |      |\n",
        "|----------------------------------------|------|------|------|------|\n",
        "| X                                      | 1    | 2    | 5    | 10    |\n",
        "| P(X)                                   | 4/10 | 3/10 | 2/10 | 1/10 |\n",
        "\n",
        "As simple as this distribution is, it tells us the probability of all possible values our random variable can take. How do we link these probabilities to the expected value and variance of $X$? From the calculation of probability we have seen earlier, we see that it is equivalent to the ratio of the number of times an outcome occurred relative to the total number of outcomes. This means that __the probability of an outcome is the weight of its contribution to the sum, hence the expected value of the random variable.__\n",
        "\n",
        "This means that knowing the probability of each value gives us a way of predicting the long-term statistics of that random variable. Let us _generate_ our own _random_ data by creating a function that generates our dart throwing example to better understand this link. Let us assume that it costs £2.00 for each dart throw and consider the question: \"Can we make a _long-term_ profit by playing this game?\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2qhkbn9khUs"
      },
      "source": [
        "# Generates random numbers between 1 and 4 from the above probability distribution\n",
        "def dart_throw(): \n",
        "    dummy_rand = np.random.randint(1, 11) # between 1 and 10\n",
        "    \n",
        "    if dummy_rand == 1:\n",
        "        return 10\n",
        "    elif dummy_rand in [2, 3]:\n",
        "        return 5\n",
        "    elif dummy_rand in [4, 5, 6]:\n",
        "        return 2\n",
        "    else:\n",
        "        return 1\n",
        "    \n",
        "print(dart_throw())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOAk1_vvkhUw"
      },
      "source": [
        "In fact, if we generate samples from our probability distribution, we see that if a large enough number of samples is generated, the theoretical probability of each outcome can be estimated with much higher accuracy from the data. Below, we generate 1000 dart throws, and calculate the probability of each outcome taking place."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlM5AV2AkhUy"
      },
      "source": [
        "# Estimated probability distribution\n",
        "n = 1000\n",
        "\n",
        "dart_throws = np.array([dart_throw() for i in range(n)])\n",
        "probability_tracker = np.zeros((1,4)) # probabilities at 0th step\n",
        "outcomes = np.zeros(4)# will keep current number of outcomes\n",
        "outcomes = {1:0, 2:0, 5:0, 10:0}\n",
        "\n",
        "# Computing the evolution of the probability of the dice rolls\n",
        "for throw in dart_throws:\n",
        "    outcomes[throw] += 1 # increase the number of the times this outcome occurred\n",
        "    new_prob = np.array(list(outcomes.values()))/sum(outcomes.values())\n",
        "    probability_tracker = np.append(probability_tracker, new_prob)\n",
        "\n",
        "throw_number = np.linspace(0, n, n+1) # number of dice rolled\n",
        "probability_tracker = np.reshape(probability_tracker, (n+1, 4))\n",
        "\n",
        "mean = np.mean(dart_throws)\n",
        "var = np.var(dart_throws)\n",
        "print(\"Expected value: \", mean)\n",
        "print(\"Variance: \", var)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9N6GzvZekhUz"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.update_layout(\n",
        "    title='Probability estimates vs number of samples',\n",
        "    xaxis_title='Number of throws(n)',\n",
        "    yaxis_title='Estimated probability',\n",
        ")\n",
        "# Adding traces\n",
        "for idx, val in enumerate(outcomes.keys()):\n",
        "    prob = probability_tracker[:, idx]\n",
        "    fig.add_trace(go.Scatter(x=throw_number, y=prob, name=str(val)))\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAV4ej6gkhU0"
      },
      "source": [
        "For now, do not worry about the code itself. In this case, we see that if we take samples that we _assume_ follow a probability distribution, the estimate of the probability of each possible outcome becomes more accurate as the number of samples increases. This means that  the more samples we have to calculate this, the more the probabilities approach the __true__ probability distribution and statistics. This is, creatively enough, known as the __law of large numbers__. Given this relationship, we can actually calculate the true expected value and variance (long-term) of any random value given its true probability distribution! \n",
        "\n",
        "$$ E(X) = \\sum_{x \\in X}xp(x) $$\n",
        "$$ Var(X) = \\sum_{x \\in X}(x-E(X))^{2}p(x)$$\n",
        "\n",
        "To consolidate these, let us compute the true expected value and variance of points with the probability distribution we have just seen:\n",
        "$$E(X) =  \\sum_{x \\in X}xp(x) = 1 \\times \\frac{4}{10} +  2\\times \\frac{3}{10} + 5\\times \\frac{2}{10} + 10 \\times \\frac{1}{10} = \\frac{30}{10} = 3$$\n",
        "$$Var(X) = \\sum_{x \\in X}(x-E(X))^{2}p(x) = (3-1)^{2} \\times \\frac{4}{10} +  (3-2)^{2}\\times \\frac{3}{10} + (5-3)^{2}\\times \\frac{2}{10} + (10-3)^{2} \\times \\frac{1}{10} = \\frac{76}{10} = 7.6$$\n",
        "\n",
        "These results are remarkably close to the mean and variance estimated from the data we generated, and would get closer and closer as we collect more samples. This allows us to answer the question asked previously, where, since on average we make £3.00 for every dart throw, then we are expected to make a profit! This shows the power of probability in statistics and data science in analyzing long-term behaviour! When we create a model based on our data, the more data we have, the _better_ the estimates made by the model will be. However, as we will see, there are multiple ways of having _better_ estimates.\n",
        "\n",
        "# Estimation: Variance & Bias\n",
        "An __estimator__ is a function of the dataset we can then use to generate an __estimate__. For example, the _sample mean_ and _sample variance_ we saw before are examples of estimators. In fact, they are a subclass known as __unbiased estimators__, meaning that after a sufficiently large number of samples, the __bias__ becomes zero. Bias is defined as the difference between the expected value of the estimate and the true value which we aim to estimate:\n",
        "$$Bias = E(\\hat{\\theta}) - \\theta$$\n",
        "\n",
        "Let us look at the example where we already have the probability distribution: the number of points we score with a dart throw. We have calculated the mean and variance to be 3 and 7.6 respectively. Let us analyse the evolution over the number of throws of the both these estimates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8LbXn8HkhU1"
      },
      "source": [
        "# Analysing evolution of sample mean and sample variance\n",
        "n = 1000\n",
        "\n",
        "dart_throws = np.array([dart_throw() for i in range(n)])\n",
        "mean_tracker = np.array([])\n",
        "var_tracker = np.array([])\n",
        "\n",
        "for i in range(n): # for each trial\n",
        "    data = dart_throws[0:i]\n",
        "    mean_tracker = np.append(mean_tracker, np.mean(data))\n",
        "    var_tracker = np.append(var_tracker, np.var(data))\n",
        "\n",
        "throw_count = np.linspace(0, n, n+1)\n",
        "true_mean = np.ones(n)*3\n",
        "true_var = np.ones(n)*7.6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTEAdefzkhU1"
      },
      "source": [
        "fig = go.Figure()\n",
        "\n",
        "fig.update_layout(\n",
        "    title='Expected value and variance (unbiased)estimates vs number of samples',\n",
        "    xaxis_title='Number of throws(n)',\n",
        "    yaxis_title='Magnitude',\n",
        ")\n",
        "# Adding traces\n",
        "fig.add_trace(go.Scatter(x=throw_count, y=mean_tracker, name='Estimated Mean'))\n",
        "fig.add_trace(go.Scatter(x=throw_count, y=var_tracker, name='Estimated Variance'))\n",
        "fig.add_trace(go.Scatter(x=throw_count, y=true_mean, name='True Mean', line=dict(dash=\"dash\")))\n",
        "fig.add_trace(go.Scatter(x=throw_count, y=true_var, name='True Variance', line=dict(dash=\"dash\")))\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWNih_4qkhU2"
      },
      "source": [
        "As predicted, after a large number of samples, the our estimates get closer and closer to their true value! When an estimator or model that is used to generate estimates shows _bias,_ even the best possible parametrisation is not optimal, and no matter how much data we obtain, our estimate will never reach the true value. When the variance of different parametrisations is high, there is a risk of over-accomodating our dataset, taking away from our estimator the ability to estimate unknown quantities. Generally, there is a trade-off between bias and variance, which will be dealt with in more detail later on in the course."
      ]
    }
  ]
}